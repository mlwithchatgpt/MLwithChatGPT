## What is LLM inference
LLM (Large Language Model) inference refers to the process of using a trained large language model, such as GPT (Generative Pre-trained Transformer), to generate text or make predictions based on input data. Inference involves feeding input text or prompts into the model and letting it produce the desired output, whether it's generating natural language text, answering questions, completing sentences, or performing other language-related tasks. During inference, the model utilizes its learned parameters to generate the most probable output given the input data.


## How does it work
LLM inference works through a process called autoregressive generation, which involves the following steps:

Tokenization: The input text is tokenized into smaller units, typically words or subwords, and then converted into numerical representations that the model can understand. Each token corresponds to a unique ID in the model's vocabulary.

Input Encoding: The tokenized input is then encoded into a format suitable for feeding into the model. This encoding often involves representing the tokens as high-dimensional vectors, which capture the semantic meaning of the words.

Model Processing: The encoded input is passed through multiple layers of the neural network model. These layers consist of transformers, which are designed to analyze and process sequential data like text. The model leverages its pre-trained parameters to understand the context of the input and predict the next token in the sequence.

Sampling: After processing the input, the model generates a probability distribution over the vocabulary for the next token. Various sampling techniques can be used to select the next token, such as greedy sampling (choosing the token with the highest probability), top-k sampling (choosing from the top k most probable tokens), or temperature sampling (adjusting the diversity of the generated text).

Output Decoding: The selected token is then appended to the input sequence, and the process iterates recursively until a predefined stopping condition is met, such as reaching a maximum sequence length or generating a special end-of-sequence token.

Post-Processing: Finally, the generated sequence of tokens is decoded back into human-readable text, which may involve reversing the tokenization process and handling special tokens or formatting.

Throughout this process, the model's learned parameters, acquired during pre-training on large text corpora, enable it to generate coherent and contextually relevant text based on the provided input. The quality of the generated text depends on various factors, including the size and quality of the training data, the architecture of the model, and the sampling strategy employed during inference.


## Given an example with GPT2
Sure! Let's say we have a GPT-2 model that has been trained on a large dataset of news articles. We want to use this model to generate a news headline based on a given prompt.

**Prompt**: "In a groundbreaking scientific discovery, researchers have found evidence of..."
**Tokenization and Encoding**: The prompt is tokenized into individual words or subwords and then encoded into numerical representations.
**Model Processing**: The encoded prompt is fed into the GPT-2 model, which analyzes the context and generates a probability distribution over the vocabulary for the next token.
**Sampling**: We use a sampling technique to select the next token. Let's say we use top-k sampling with k=5, which means we choose from the top 5 most probable tokens. The model might output probabilities like: "evidence" (0.35), "life" (0.25), "water" (0.15), "new" (0.1), "space" (0.05). We randomly select one of these tokens based on their probabilities.
**Output Decoding**: The selected token is appended to the prompt, and the process repeats until we reach a predefined stopping condition (e.g., reaching a maximum sequence length or generating a special end-of-sequence token).
**Post-Processing**: Once we have generated a sequence of tokens, we decode it back into human-readable text. For example, the model might generate the headline: "In a groundbreaking scientific discovery, researchers have found evidence of water on Mars."
This example demonstrates how the GPT-2 model can generate coherent and contextually relevant text based on a given prompt by leveraging its learned parameters and autoregressive generation process.

## Code
Certainly! Here's a Python code example using the transformers library to generate text with GPT-2:

```
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# Load pre-trained model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Set the model to evaluation mode
model.eval()

# Set a seed for reproducibility (optional)
torch.manual_seed(42)

# Define a prompt
prompt = "In a groundbreaking scientific discovery, researchers have found evidence of"

# Tokenize the prompt
input_ids = tokenizer.encode(prompt, return_tensors="pt")

# Generate text based on the prompt
output = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=0.7)

# Decode the generated output
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

# Print the generated text
print("Generated Text:")
print(generated_text)
```
This code snippet demonstrates how to use the `transformers` library to generate text with GPT-2. It loads the pre-trained GPT-2 tokenizer and model, sets them to evaluation mode, defines a prompt, tokenizes the prompt, generates text based on the prompt, decodes the generated output, and finally prints the generated text.

Make sure you have the transformers library installed (`pip install transformers`) and appropriate GPU resources if available for faster processing.



## What is temperature
Temperature is a parameter used in sampling-based generation algorithms, such as those used in language models like GPT-2. It controls the diversity of the generated text by adjusting the softmax function that converts logits (the output of the model) into a probability distribution over the vocabulary.

In sampling-based generation, after the model produces logits for the next token, these logits are divided by the temperature parameter before applying the softmax function. A higher temperature increases the entropy of the distribution, making it more likely to choose tokens with lower probabilities. Conversely, a lower temperature concentrates the probability mass on tokens with higher probabilities, resulting in more conservative and deterministic sampling.

Here's how temperature affects the sampling process:

High Temperature: When temperature is high (e.g., >1), it encourages more diverse and exploratory sampling. The model is more likely to choose tokens with lower probabilities, resulting in more varied and creative outputs. However, this can also lead to less coherent or grammatically correct text.

Low Temperature: When temperature is low (e.g., <1), it encourages more conservative sampling. The model is more likely to choose tokens with higher probabilities, leading to more deterministic and predictable outputs. This tends to produce more coherent and grammatically correct text but may lack creativity or variety.

In practice, the choice of temperature depends on the specific application and desired trade-off between diversity and quality in the generated text. A temperature close to 1 is often used as a balanced default, but it can be adjusted based on the characteristics of the task and the preferences of the user.

## Give the mathematical expression


The mathematical expression for adjusting temperature in sampling-based generation is as follows:

Given a set of logits $z$ representing the output of the model for the next token, the softmax function with temperature $\tau$ is applied to obtain a probability distribution $p$ over the vocabulary:
    $$p_i = \frac{\exp(z_i/\tau)}{\sum_i \exp(z_i/\tau)}$$
    where 
    * $p_i$ is the probability of the $i$th token in the vocabulary,
    * $z_i$ is the logit corresponding to the $i$th token,
    * $\tau$ is the temperature parameter.
When $\tau=1$, the temperature has no effect, and the softmax function behaves as usual. As $\tau$ approaches zero, the distribution becomes more peaked, favoring tokens with higher logits. Conversely, as $\tau$ increases, the distribution becomes more uniform, allowing tokens with lower logits to have higher probabilities.

After obtaining the probability distribution $p$, tokens are sampled from this distribution to determine the next token in the generated sequence. Adjusting the temperature allows controlling the balance between exploration (diversity) and exploitation (quality) in the generated text.

