{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3969419,"sourceType":"datasetVersion","datasetId":2355852}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training/fine-tuning of GPT2\n\nUse Penn-treebank-dataset on kaggle\n\nIf you want to train a network from scratch, you may use \n```\nconfig = GPT2Config(vocab_size = 50257, n_positions = 1024, n_embd = 768, n_layer = 12, n_head = 12)\nmodel = GPT2LMHeadModel(config)\n```","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/penn-treebank-dataset /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-04-12T04:04:31.267240Z","iopub.execute_input":"2024-04-12T04:04:31.268069Z","iopub.status.idle":"2024-04-12T04:04:32.363350Z","shell.execute_reply.started":"2024-04-12T04:04:31.268034Z","shell.execute_reply":"2024-04-12T04:04:32.362040Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\n\n\n# Load pre-trained GPT-2 tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n# Load and preprocess the Wikitext dataset\ntrain_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=\"/kaggle/working/penn-treebank-dataset/ptbdataset/ptb.train.txt\",  # specify the path to Wikitext train dataset\n    block_size=128  # adjust according to your computational resources\n)\neval_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=\"/kaggle/working/penn-treebank-dataset/ptbdataset/ptb.test.txt\",  # specify the path to Wikitext validation dataset\n    block_size=128  # adjust according to your computational resources\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T04:04:59.390312Z","iopub.execute_input":"2024-04-12T04:04:59.390719Z","iopub.status.idle":"2024-04-12T04:05:11.160467Z","shell.execute_reply.started":"2024-04-12T04:04:59.390686Z","shell.execute_reply":"2024-04-12T04:05:11.159516Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-12T04:05:12.025631Z","iopub.execute_input":"2024-04-12T04:05:12.026286Z","iopub.status.idle":"2024-04-12T04:05:12.033499Z","shell.execute_reply.started":"2024-04-12T04:05:12.026255Z","shell.execute_reply":"2024-04-12T04:05:12.032487Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<transformers.data.datasets.language_modeling.TextDataset at 0x7a3dd3ebcf70>"},"metadata":{}}]},{"cell_type":"code","source":"# Prepare training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,  # adjust based on your GPU memory\n    per_device_eval_batch_size=4,  # adjust based on your GPU memory\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_steps=500,\n    eval_steps=500,\n    evaluation_strategy=\"steps\",\n    save_total_limit=2,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T04:05:56.005180Z","iopub.execute_input":"2024-04-12T04:05:56.005508Z","iopub.status.idle":"2024-04-12T04:05:56.013664Z","shell.execute_reply.started":"2024-04-12T04:05:56.005483Z","shell.execute_reply":"2024-04-12T04:05:56.012580Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Prepare data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T04:06:09.148312Z","iopub.execute_input":"2024-04-12T04:06:09.149220Z","iopub.status.idle":"2024-04-12T04:06:10.512337Z","shell.execute_reply.started":"2024-04-12T04:06:09.149187Z","shell.execute_reply":"2024-04-12T04:06:10.511581Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\ntrainer.train()\n\n# Evaluate the model\ntrainer.evaluate()\n\n# Save the model\ntrainer.save_model(\"./gpt2-trained\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T04:06:24.653693Z","iopub.execute_input":"2024-04-12T04:06:24.654321Z","iopub.status.idle":"2024-04-12T04:25:01.905699Z","shell.execute_reply.started":"2024-04-12T04:06:24.654288Z","shell.execute_reply":"2024-04-12T04:25:01.904654Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240412_040654-fyaql36n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mlwithchatgpt/huggingface/runs/fyaql36n/workspace' target=\"_blank\">major-wind-4</a></strong> to <a href='https://wandb.ai/mlwithchatgpt/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mlwithchatgpt/huggingface' target=\"_blank\">https://wandb.ai/mlwithchatgpt/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mlwithchatgpt/huggingface/runs/fyaql36n/workspace' target=\"_blank\">https://wandb.ai/mlwithchatgpt/huggingface/runs/fyaql36n/workspace</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3444' max='3444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3444/3444 17:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.190000</td>\n      <td>3.030819</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.046500</td>\n      <td>2.940042</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.915200</td>\n      <td>2.901784</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.890500</td>\n      <td>2.874459</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.783400</td>\n      <td>2.861092</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.766500</td>\n      <td>2.852276</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [103/103 00:10]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"model.to('cuda')\ndef generate_text(prompt, model, max_length=100, temperature=1.0, top_k=50, device='cuda'):\n    # Load pre-trained tokenizer and model\n\n    # Tokenize input prompt\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # Generate text based on prompt\n    output = model.generate(\n        input_ids,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        pad_token_id=tokenizer.eos_token_id,\n        num_return_sequences=1\n    )\n\n    # Decode generated tokens back to text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    return generated_text\n\n# Example prompt\nprompt = \"new york stock exchange\"\n\n# Generate text using GPT-2 model\ngenerated_text = generate_text(prompt, model, max_length=50, temperature=0.7, top_k=30, device='cuda')\n\nprint(\"Generated text:\")\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T04:25:01.907375Z","iopub.execute_input":"2024-04-12T04:25:01.907665Z","iopub.status.idle":"2024-04-12T04:25:03.843216Z","shell.execute_reply.started":"2024-04-12T04:25:01.907640Z","shell.execute_reply":"2024-04-12T04:25:03.842065Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated text:\nnew york stock exchange composite trading yesterday \n the dow jones industrial average closed at N \n the dow jones industrial average closed at N \n the dow jones industrial average closed at N \n the dow jones industrial average closed\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
