## What is the structure of GPT2

GPT-2, or Generative Pre-trained Transformer 2, is a deep learning model architecture based on the Transformer architecture. It consists of multiple layers of self-attention mechanisms and feedforward neural networks. Here's a simplified overview of its structure:

1. **Input Embedding Layer**: Converts input tokens into high-dimensional embeddings.

2. **Transformer Encoder**: GPT-2 uses a transformer encoder architecture, which includes multiple layers of:

    * **Multi-head Self-Attention Mechanism**: Each token attends to all other tokens in the sequence to capture contextual relationships.
    * **Position-wise Feedforward Networks**: After attention, each token representation passes through a position-wise fully connected feedforward neural network.
    * **Layer Normalization**: Applies normalization to stabilize training.
3. **Decoder Head (Optional)**: Unlike traditional transformers which often include a decoder for tasks like language generation, GPT-2 operates as a decoder-only model, meaning it's typically used for autoregressive tasks where it generates outputs token by token.

4. **Output Projection Layer**: Maps the final hidden states of the model back to the vocabulary space, generating a probability distribution over the vocabulary for each token.

GPT-2 comes in various sizes, ranging from small to extra-large, with the larger models having more layers, parameters, and computational requirements. The model is trained on vast amounts of text data using unsupervised learning techniques, enabling it to generate coherent and contextually relevant text.


## Code
Sure, here's a basic example of how you can implement a simple version of the GPT-2 architecture using Python and the PyTorch library:
```
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        
        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values, keys, query, mask):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # Split the embedding into self.heads different pieces
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)

        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)

        energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys]) # batched matrix multiplication
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20")) # making the output value -1e20 where there is a 0 in the mask, this will be used later when applying softmax to not consider the -1e20 values
        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1/2)), dim=3)
        out = torch.einsum("nhql,nlhd->nqhd", [attention, values]).reshape(N, query_len, self.heads*self.head_dim)

        return self.fc_out(out)


class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        self.attention = SelfAttention(embed_size, heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)

        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        # Add skip connection, run through normalization and dropout
        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out


class GPT2(nn.Module):
    def __init__(
        self,
        embed_size,
        heads,
        num_layers,
        forward_expansion,
        vocab_size,
        max_length,
        dropout,
    ):
        super(GPT2, self).__init__()
        self.embed_size = embed_size
        self.token_embedding = nn.Embedding(vocab_size, embed_size)
        self.positional_embedding = nn.Embedding(max_length, embed_size)
        self.layers = nn.ModuleList(
            [
                TransformerBlock(embed_size, heads, dropout, forward_expansion)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, vocab_size)
        self.dropout = nn.Dropout(dropout)
        self.max_length = max_length

    def forward(self, x, mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length)

        out = self.dropout(
            (self.token_embedding(x) + self.positional_embedding(positions))
        )

        for layer in self.layers:
            out = layer(out, out, out, mask)

        out = self.fc_out(out)
        return out
# Example usage
vocab_size = 10000  # Example vocabulary size
d_model = 512  # Dimensionality of token embeddings and transformer layers
n_head = 8  # Number of attention heads
num_layers = 6  # Number of transformer blocks
forward_expansion = 4 # Multiple of FFN
max_length = 100 # Maximum of the input sequence length

model = GPT2(d_model, n_head, num_layers, forward_expansion, vocab_size, max_length=max_length, dropout=0.)
input_sequence = torch.tensor([[1, 2, 3, 4, 5]])  # Example input sequence
output = model(input_sequence)
print("Output shape:", output.shape)  # Shape: [batch_size, sequence_length, vocab_size]

```

## Explain the self-attention

Self-attention is a mechanism that enables neural networks to weigh the importance of different parts of their input when making predictions or generating outputs. It's a key component of transformer architectures like GPT-2. Here's an explanation of how self-attention works:

1. **Key, Query, Value**: In self-attention, each input token is associated with three vectors: Key, Query, and Value. These vectors are derived from the input embeddings through learned linear transformations. The Key vectors are used to compute compatibility scores with other tokens, the Query vectors are used to generate attention weights, and the Value vectors represent the information associated with each token.

2. **Attention Scores**: To compute the attention scores, the Query vector of a token is compared with the Key vectors of all tokens using some measure of similarity, often the dot product. This results in a score for each token, indicating how much attention the current token should pay to the others. These scores are computed for all tokens simultaneously, allowing for parallelization.

3. **Attention Weights**: The attention scores are then passed through a softmax function to obtain attention weights. These weights determine how much each token contributes to the representation of the current token. Tokens with higher attention scores receive higher weights, indicating that they are more relevant to the current token.

4. **Contextual Representation**: Finally, the weighted sum of the Value vectors, weighted by the attention weights, is computed. This results in a contextual representation of the current token, taking into account the information from all other tokens in the sequence. This contextual representation is then used as input to subsequent layers in the neural network.

By allowing each token to attend to all other tokens in the sequence, self-attention captures long-range dependencies and contextual information effectively, making it well-suited for tasks involving sequential data like natural language processing. Additionally, self-attention is inherently parallelizable, making it efficient to compute even for long sequences.

## Mathematical expression

Sure, let's break down the mathematical expressions involved in self-attention:

Given:
* $Q$: Query matrix of shape ($N$, seq_length, embed_size)
* $K$: Key matrix of shape ($N$, seq_length, embed_size)
* $V$: Value matrix of shape ($N$, seq_length, embed_size)
* mask: Masking matrix of shape ($N$, 1, seq_length, seq_length) for handling padded sequences

1. **Compute Attention Scores** (energy):
$$\text{energy} = Q \cdot K^T$$
Here, $\cdot$ represents matrix multiplication and $K^T$ is the transpose of the Key matrix. This operation results in a matrix of attention scores of shape 
($N$,seq_length,seq_length), where each row corresponds to the attention scores of a particular query token across all keys.

2. **Apply Masking (Optional)**:
If masking is applied to handle padded sequences, we apply it here by adding a large negative value (-inf) to the masked positions in the attention scores:
$$\text{energy} = \text{energy} + \text{mask}$$

This ensures that masked positions receive very low attention scores after the softmax operation.

3. **Compute Attention Weights**:

$$\text{attention} = \text{softmax}\left(\frac{\text{energy}}{\sqrt d_k}\right)$$

Here, `softmax` is applied along the last dimension (token dimension) of the energy matrix. $d_k$  represents the dimension of the Key vectors, usually the embedding size.

5. **Compute Contextual Representation**:
$$\text{out} = \text{attention}\cdot V$$
This step computes the weighted sum of Value vectors based on the attention weights. The resulting matrix `out` has the same shape as the Value matrix and represents the contextualized representations of the input tokens.

This mathematical formulation captures the essence of self-attention, allowing the model to efficiently attend to relevant information across different positions in the sequence.


## This is attention not self-attention
You're correct, let's delve deeper into the mathematical expression specifically for self-attention, which involves the concepts of Query, Key, and Value, and how they are utilized within the self-attention mechanism.

Given:
* $X$: Input token embeddings of shape 
* $W_Q$: Weight matrix for the Query transformation
* $W_K$: Weight matrix for the Key transformation
* $W_V$: Weight matrix for the Value transformation

1. **Query, Key, Value Transformations**:

We apply linear transformations to the input embeddings to obtain Query ($Q$), Key (
$K$), and Value ($V$V) matrices:
$$Q=X\cdot W_Q, \quad K = X\cdot W_K,\quad V=X\cdot W_V$$
Here $\cdot$ denotes matrix multiplication.

2. **Compute Attention Scores** (energy):
We compute the dot product of Query and Key matrices, scaled by the square root of the dimension of the Key vectors ($\sqrt{d_k}$):
$$\text{energy} = \frac{Q\cdot K^T}{\sqrt d_k}$$

This operation results in a matrix of attention scores of shape ($N$,seq_length,seq_length), where each row corresponds to the attention scores of a particular token across all other tokens.

3. **Apply Softmax**:
We apply the softmax function along the last dimension to obtain attention weights (attention):
$$\text{attention}_{i,j} = \text{softmax} (\text{energy}_{i,j}) = \frac{\text{energy}_{i,j}}{\sum_{k=1}^{\text{seqlength}} \exp (\text{energy}_{i,k})}$$

This ensures that the attention weights sum up to 1 across all tokens.

4. **Compute Contextual Representation**:
We compute the weighted sum of Value vectors based on the attention weights:

$$\text{out}_i = \sum_{j=1}^{\text{seqlength}} \text{attention}_{i,j}\cdot V_{i,j}$$
This step generates the contextual representation of each token, where $\text{out}_i$​ represents the contextualized representation of the 
$i$-th token.

This formulation represents the core operations of self-attention, where each token's representation is adjusted based on its relationship with other tokens in the sequence, allowing the model to capture dependencies and relationships effectively.

## What is multihead attention


Multi-head attention is an extension of the self-attention mechanism commonly used in transformer architectures like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers). It allows the model to focus on different parts of the input sequence simultaneously, capturing different aspects of the data's relationships.

In multi-head attention, the attention mechanism is applied multiple times in parallel, with each instance being called a "head". Each head operates on different linearly projected versions (queries, keys, and values) of the input embeddings. After each head computes its own attention scores, the results are concatenated and linearly transformed to obtain the final multi-head attention output.

Here's a high-level overview of the steps involved in multi-head attention:

1. **Linear Projection**: The input embeddings are linearly projected into multiple sets of queries, keys, and values. For example, if we have $H$ heads, we project the embeddings into $H$ sets of query, key, and value matrices.

2. **Parallel Attention Heads**: Each head independently computes attention scores using its own set of query, key, and value matrices. This means that each head can focus on different aspects of the input sequence.

3. **Concatenation and Linear Transformation**: After computing attention scores for each head, the results are concatenated along the last dimension and linearly transformed to obtain the final multi-head attention output. This ensures that the information from all heads is combined effectively.

4. **Final Output**: The concatenated and linearly transformed output is the final output of the multi-head attention mechanism, which is then typically passed through additional layers in the transformer architecture.

By allowing the model to attend to different parts of the input sequence in parallel, multi-head attention enables the model to capture a richer set of relationships and dependencies, leading to improved performance on various natural language processing tasks. Additionally, multi-head attention enhances the model's interpretability, as each head can be interpreted as focusing on a different aspect of the input data.

## Mathematical formula
The mathematical formulation of multi-head attention involves several steps, including linear projections, computing attention scores, concatenation, and linear transformations. Let's break down the mathematical expressions for multi-head attention:

Given:

* $X$: Input token embeddings of shape ($N$, seq_length,
embed_size)
* $W_Q^i$: Weight matrix for the Query transformation for the $i$-th head
* $W_K^i$: Weight matrix for the Key transformation for the $i$-th head
* $W_V^i$: Weight matrix for the Value transformation for the $i$-th head
* $W_O^i$: Weight matrix for the output linear transformation
* $H$: Number of attention heads

```
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
```

1. **Linear Projection**:
Linearly project the input embeddings into multiple sets of queries, keys, and values for each head:
$$Q_i = X\cdot W_Q^i, K_i = X\cdot W_K^i, V_i = X\cdot W_V^i,~\text{for } i=1,2,\ldots, H$$
Here $Q_i, K_i$ and $V_i$ represent the query, key, and value matrices for the $i$-th head, respectively.
```
        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)
```

2. **Compute Attention Scores**:
For each head, compute the attention scores using the query and key matrices:
$$\text{energy}_i = \frac{Q_i\cdot K_i^T}{\sqrt{d_k}}$$
Here, $\text{energy}_i$ represents the attention scores for the $i$-th head, and $d_k$ is the dimension of the Key vectors.
```
energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])
```

3. **Apply Softmax**:
Apply the softmax function along the last dimension to obtain attention weights for each head:

$$\text{attention}_i = \text{softmax}(\text{energy}_i) = \frac{\exp(\text{energy}_i)}{\sum_{j=1}^{\text{seq_length}}\exp(\text{energy}_{i})}$$

```
        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1/2)), dim=3)
```
4. **Compute Contextual Representation**:
For each head, compute the weighted sum of Value vectors based on the attention weights:
$$\text{out}_i = \text{attention}_i\cdot V_i$$
```
        out = torch.einsum("nhql,nlhd->nqhd", [attention, values]).reshape(N, query_len, self.heads*self.head_dim)
```
5. **Concatenation**:
Concatenate the outputs of all heads along the last dimension:
$$\text{out} = \text{Concatenate}([\text{out}_1, \text{out}_2, \ldots, \text{out}_H])$$

6. **Linear Transformation**:
Apply a linear transformation to the concatenated output: $$\text{finalout} = \text{out} \cdot W_O$$

```
self.fc_out(out)
```
This formulation allows the model to capture different aspects of the input sequence in parallel across multiple attention heads, leading to richer representations and improved performance on various tasks.


## What is mask

In the context of attention mechanisms, particularly in transformer architectures like GPT and BERT, a mask is a binary matrix used to control which elements in the input sequence are attended to during the computation of attention scores. Masks are primarily used to handle variable-length sequences and prevent the model from attending to padding tokens or future tokens during training or inference.

There are different types of masks used in transformer models, including:

**Padding Mask**: A padding mask is used to indicate which elements in the input sequence are padding tokens. Padding tokens are often added to sequences of varying lengths to make them uniform for efficient batch processing. By applying a padding mask, the attention mechanism ensures that padding tokens are not attended to, preventing the model from making predictions based on irrelevant information.

**Look-ahead Mask**: A look-ahead mask is used in auto-regressive models, such as GPT, to prevent tokens from attending to future tokens in the sequence during training. In other words, it masks out the upper triangular portion of the attention matrix, ensuring that each token attends only to tokens preceding it in the sequence. This prevents information leakage from future tokens and helps maintain the autoregressive property of the model.

**Decoder Mask**: In sequence-to-sequence models, such as the decoder in the Transformer architecture, a decoder mask is used to prevent tokens in the decoder from attending to subsequent tokens in the decoder. This ensures that during the generation of the output sequence, each token only attends to tokens that have already been generated.

Masks are typically binary matrices of shape ($N$, seq_length,seq_length)$, where $N$ is the batch size and 
`seq_length` is the length of the input sequence. Masks are applied to the attention scores before the softmax operation, setting the masked elements to a large negative value (e.g., $-\infty$) to ensure that they are effectively ignored during normalization.

Overall, masks are essential components of transformer architectures, allowing the model to focus on relevant information and improve its performance on various tasks involving sequential data.

## Example of look ahead mask

Let's consider an example of a look-ahead mask used in an auto-regressive model, such as the decoder in the Transformer architecture. The purpose of the look-ahead mask is to prevent tokens from attending to future tokens in the sequence during training, maintaining the autoregressive property of the model.

Suppose we have a sequence of tokens:
```
Sequence: [8, 3, 7, 2, 5]
```
The look-ahead mask for this sequence will be a binary matrix where the upper triangular portion is masked out, indicating that each token can only attend to tokens preceding it in the sequence.

The look-ahead mask for this sequence would look like this:
```
Look-ahead Mask:
[
  [0, 1, 1, 1, 1],
  [0, 0, 1, 1, 1],
  [0, 0, 0, 1, 1],
  [0, 0, 0, 0, 1],
  [0, 0, 0, 0, 0]
]
```
In this mask:

* All elements in the upper triangular portion of the matrix are set to 1, indicating that each token can attend to all tokens preceding it in the sequence.
* All elements in the lower triangular portion and the diagonal are set to 0, indicating that each token cannot attend to tokens that come after it in the sequence.

During the computation of attention scores, this look-ahead mask is applied element-wise to ensure that each token attends only to tokens preceding it in the sequence, preventing information leakage from future tokens during training. This helps maintain the autoregressive property of the model and improves its performance on tasks like language modeling and sequence generation.

## What is LayerNorm
Layer normalization (LayerNorm) is a technique used in deep neural networks to normalize the activations of each neuron across the features dimension. It is applied independently to each example in a mini-batch, enhancing the stability and convergence of the training process. LayerNorm is a popular choice in transformer architectures like BERT and GPT.

Here's how LayerNorm works:

1. **Normalization**: For each example $x$
 in the mini-batch, LayerNorm computes the mean 
$\mu$ and standard deviation 
$\sigma$ of the activations across the feature dimension.

2. **Centering and Scaling**: LayerNorm then subtracts the mean $\mu$ from each activation and divides by the standard deviation $\sigma$. Additionally, it applies learnable parameters $\gamma$ (scaling) and 
$\beta$ (shifting) to each normalized activation. These parameters are learned during the training process.

3. **Activation**: The normalized and scaled activations are then passed through an activation function, typically ReLU (Rectified Linear Unit), although other activation functions like tanh or sigmoid can also be used.

Mathematically, the LayerNorm operation for a given input 
$x$ can be represented as follows:
$$\text{LayerNorm}(x) = \gamma\cdot \frac{x-\mu}{\sqrt{\sigma^2+\varepsilon}} + \beta$$
where 
* $x$ is the input tensor
* $\mu$ is the mean of the activations.
* $\sigma$ is the standard deviation of the activations.
* $\gamma$ is the learnable scaling parameter (initialized as ones).
* $\beta$ is the learnable shifting parameter (initialized as zeros).
* $\varepsilon$ is a small constant (usually $10^{-5}$) added to the denominator for numerical stability.

LayerNorm helps address the problem of internal covariate shift, where the distribution of activations changes significantly during training, slowing down convergence. By normalizing activations across the features dimension, LayerNorm ensures that the model learns more robust and stable representations, leading to improved performance on various tasks.

## Code
Here's a simple implementation of LayerNorm in PyTorch:
```
import torch
import torch.nn as nn

class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(features))
        self.beta = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        normalized_x = (x - mean) / (std + self.eps)
        return self.gamma * normalized_x + self.beta
```



## model config for GPT2

```
from transformers import GPT2Config

# Example configuration for GPT-2 medium
config = GPT2Config(
    vocab_size=50257,  # Size of the vocabulary
    n_positions=1024,  # Maximum sequence length
    n_ctx=1024,        # Maximum context size for attention mechanism
    n_embd=1024,       # Dimensionality of the embeddings and hidden states
    n_layer=24,        # Number of transformer layers
    n_head=16,         # Number of attention heads
    resid_pdrop=0.1,   # Dropout probability for residual connections
    embd_pdrop=0.1,    # Dropout probability for the embeddings
    attn_pdrop=0.1,    # Dropout probability for the attention mechanism
    layer_norm_epsilon=1e-5,  # Epsilon value for layer normalization
)

# Alternatively, you can also load a pre-trained configuration
config = GPT2Config.from_pretrained("gpt2-medium")
```
In this example, we define a configuration for the GPT-2 medium model with specific parameter values. You can adjust these parameters based on your requirements. Alternatively, you can also load a pre-trained configuration using the from_pretrained method.

Here are some key parameters in the GPT2Config:

* `vocab_size`: Size of the vocabulary.
* `n_positions`: Maximum sequence length or number of positional embeddings.
* `n_ctx`: Maximum context size for the attention mechanism.
* `n_embd`: Dimensionality of the embeddings and hidden states.
* `n_layer`: Number of transformer layers.
* `n_head`: Number of attention heads.
* `resid_pdrop`: Dropout probability for residual connections.
* `embd_pdrop`: Dropout probability for the embeddings.
* `attn_pdrop`: Dropout probability for the attention mechanism.
* `layer_norm_epsilon`: Epsilon value for layer normalization.
You can adjust these parameters based on the specific requirements of your task or experiment.


## What is positional encoding 
Positional encoding is a technique used in sequence-to-sequence models, particularly in the context of transformer-based architectures like the Transformer model. In natural language processing tasks, such as machine translation or text generation, it's essential to maintain the order of words in a sequence.

In a transformer model, the input sequence is fed into the network as embeddings, where each token in the sequence is represented as a vector. However, since transformers do not inherently understand the order of the tokens, positional encoding is used to provide the model with information about the position of each token in the sequence.

The most common method of positional encoding involves adding fixed-length vectors to the input embeddings. These vectors encode information about the position of tokens relative to each other in the sequence. They are typically designed so that the model can easily learn to attend to the relative positions of tokens during processing.

The positional encodings are usually calculated based on mathematical functions such as sine and cosine functions. This allows the model to capture the sequential order of tokens without relying solely on the input embeddings themselves. By incorporating positional encoding, transformer models can effectively handle sequences of variable length and understand the sequential nature of the input data.


## Example
Sure, let's illustrate positional encoding with a simple example. Suppose we have the following input sequence of words:

"Hello, how are you?"

We first convert each word into its corresponding word embedding using a pre-trained word embedding model. Let's say each word is represented by a 4-dimensional embedding vector for simplicity:

"Hello" -> [0.1, 0.2, 0.3, 0.4]
"how" -> [0.2, 0.3, 0.4, 0.5]
"are" -> [0.3, 0.4, 0.5, 0.6]
"you" -> [0.4, 0.5, 0.6, 0.7]

Now, we need to add positional encoding to these embeddings to convey their positions in the sequence. One common method of positional encoding is using sine and cosine functions. For each position in the sequence and each dimension of the embedding, we compute the positional encoding as follows:

$$\text{PE}_{pos,2i} = \sin \frac{pos}{10000^{2i/d_{model}}}$$
$$\text{PE}_{pos,2i} = \cos \frac{pos}{10000^{2i/d_{model}}}$$

    Where:
    * $pos$ is the position of the token in the sequence.
    * $i$ is the dimension of the positional encoding.
    * $d_{model}$ is the dimensionality of the model (i.e., the dimensionality of the embeddings).
    
For example, let's assume the dimensionality of the model (d_model) is 4. We'll compute the positional encoding for each position in the sequence:

Position 1:
    $$\text{PE}_{(1,0)} = \sin(1/10000^{1/4})$$ 
    $$\text{PE}_{(1,1)} = \sin(1/10000^{1/4})$$ 
    $$\text{PE}_{(1,2)} = \sin(1/10000^{2/4})$$ 
    $$\text{PE}_{(1,3)} = \sin(1/10000^{2/4})$$ 

Similarly, we compute positional encodings for positions 2, 3, and 4.

Finally, we add these positional encodings to the corresponding word embeddings:

For example, for position 1:

"Hello" + $\text{PE}_{(1,0)},\text{PE}_{(1,1)}, \text{PE}_{(1,2)}, \text{PE}_{(1,3)}$

This way, the model receives both the word embeddings and the positional information, enabling it to understand the sequential order of the input tokens.

## Code 
```
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=0.1)
        
        # Compute positional encodings
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        # Add positional encoding to input tensor
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

# Example usage
d_model = 512  # Dimensionality of the model
max_len = 1000  # Maximum sequence length

pos_encoder = PositionalEncoding(d_model, max_len)

# Example input tensor (batch_size=10, sequence_length=100, d_model=512)
input_tensor = torch.randn(10, 100, d_model)

# Add positional encoding to input tensor
output_tensor = pos_encoder(input_tensor)

print("Input Tensor Shape:", input_tensor.shape)
print("Output Tensor Shape:", output_tensor.shape)

```

## Why sin positional encoding
The sine and cosine positional encodings used in transformers have a key property: they produce unique values for each position in the sequence. This property is crucial for providing the model with information about the relative positions of tokens in the sequence.

The use of sine and cosine functions in positional encoding allows the model to capture different frequencies of positional information. Specifically, the sine function captures the "up and down" patterns of positions, while the cosine function captures the "zigzag" patterns. Combining both sine and cosine functions ensures that the positional encodings are diverse and can represent various positional relationships within the sequence.

Furthermore, sine and cosine functions are continuous and periodic, which means they can represent positional information for sequences of arbitrary length. This property is important because transformer models can handle input sequences of variable length.

Overall, the choice of sine and cosine functions for positional encoding enables the model to effectively learn and understand the sequential order of tokens in the input sequence, which is essential for tasks such as natural language processing and sequence generation.












